:PROPERTIES:
:ID:       3774439d-af75-453e-b3e9-9d578b6bec46
:mtime:    20251212133522 20250626215633 20250622203534 20250622162759 20250603081658 20250403122443 20250328170850 20250223131954 20250208103833 20250206173224 20250206100320 20241128103141 20241117170240 20241114174544 20230911222107 20230528222513 20230215120711
:ctime:    20230215120711
:END:
#+TITLE: Nginx
#+FILETAGS: :gnu:linux:nginx:web:

* Installation

Available across packages install as appropriate to your OS.

#+begin_src bash
  emerge -av nginx        # Gentoo
  pacman -Syu nginx       # Arch
  apt-get install nginx   # Ubuntu
#+end_src


* Configuration

** General

*** Block "AI" Bots

I wanted to block AI bots on my self-hosted sites. Thanks to [[https://rknight.me/blog/blocking-bots-with-nginx/][Blocking Bots with Nginx by Robb Knight]] this was pretty
straight-forward. I didn't have the same strict requirements as the author of the article so I created
~/etc/nginx/redirect.conf~  and added to it the following entry which is case-insensitive list of AI Bot names from
[[https://github.com/ai-robots-txt/ai.robots.txt/blob/main/robots.txt][here]].

#+begin_src
# List from https://github.com/ai-robots-txt/ai.robots.txt/blob/main/robots.txt
if ($http_user_agent ~* "(AI2Bot|Ai2Bot-Dolma|Amazonbot|anthropic-ai|Applebot|Applebot-Extended|Bytespider|CCBot|ChatGPT-User|Claude-Web|ClaudeBot|cohere-ai|Diffbot|DuckAssistBot|FacebookBot|facebookexternalhit|FriendlyCrawler|Google-Extended|GoogleOther|GoogleOther-Image|GoogleOther-Video|GPTBot|iaskspider/2.0|ICC-Crawler|ImagesiftBot|img2dataset|ISSCyberRiskCrawler|Kangaroo Bot|Meta-ExternalAgent|Meta-ExternalFetcher|OAI-SearchBot|omgili|omgilibot|PerplexityBot|PetalBot|Scrapy|Sidetrade indexer bot|Timpibot|VelenPublicWebCrawler|Webzio-Extended|YouBot)"){
    return 307 https://ash-speed.hetzner.com/10GB.bin;
}
#+end_src


Any bot that matches the above "user agent" will get redirected to ~https://ash-speed.hetzner.com/10GB.bin~.

For each ~server{...}~ entry under ~/etc/nginx/nginx.conf~ I then added the following line to include the above file.

#+begin_src
        include /etc/nginx/redirect.conf
#+end_src


Restart the server and test by changing the "user agent" in any browser to one that matches the above and I was asked if
I wanted to download a ~10GB.bin~ file.

*** Serve AI Bots Junk

Came across [[https://zadzmo.org/code/nepenthes/][Nepenthes]] which infinitely generates junk pages when web-crawlers hit it. Not yet deployed this.

This isn't the only approach its possible [[https://tzovar.as/algorithmic-sabotage-ii/][Algorithmic sabotage for static sites II: Images]] too.

*** Codeberg/GitHub static stites

You can also serve up junk on static sites that are hosted on Codeberg/GitHub pages using the approach described in
[[\[\[https://tzovar.as/algorithmic-sabotage-ssg/\]\[Bastian Greshake Tzovaras · Algorithmic sabotage for static
sites\]\]][Algorithmic sabotage for static sites]]. It uses the [[https://marcusb.org/hacks/quixotic.html][Quix[]otic]] tool to achieve this.

*** Iocaine

A great [[https://vulpinecitrus.info/blog/guarding-git-forge-ai-scrapers/][article]] ([[https://web.archive.org/web/20251212132351/https://vulpinecitrus.info/blog/guarding-git-forge-ai-scrapers/][archive copy]]) showing how to use [[https://iocaine.madhouse-project.org/documentation/3/][Iocaine3]] to serve garbage to crawlers using Nginx for reverse proxying.

*** Poison Fountain

[[https://rnsaffn.com/poison3/][Poison Fountain]] is another attempt to feed AI web-crawlers a bunch of junk (see also [[https://www.theregister.com/2026/01/11/industry_insiders_seek_to_poison/][Register article]]).

** LetsEncrypt

I use LetsEncrypt to provide certificates for my sites so they are secure. Using Nginx when renewing remember to include
the ~--nginx~ flag.

#+begin_src
certbot --nginx renew
#+end_src

A useful tool for debugging why certificate issue may fail is [[https://letsdebug.net/][Let's Debug]]

In 2025 LetsEncrypt started [[https://community.letsencrypt.org/t/getting-ready-to-issue-ip-address-certificates/][issuing certificates for IP addresses]] which is really useful for those seeking to encourage
adoption of the [[https://ar.al/2025/06/25/web-numbers/][SmallWeb]].

* Servers

** [[https://wiki.nshephard.dev][Dokuwiki]]

I use [[id:bc096b27-5f0e-426c-9722-7798e12ca2dc][Dokuwiki]] to host a couple of wiki's as its simple and doesn't require a database.

** [[https://freshrss.nshephard.dev][FreshRSS]]

** [[https://opengist.nshephard.dev][OpenGist]]

This runs locally and uses a reverse proxy to connect to it from the outside world. It plays nicely with ForgeJo and
allows used of its login options.

** [[https://forgejo.nshephard.dev][Forgejo]]

** [[id:d9c960c2-71b6-45e6-b388-dcd07b9da3e1][FitTrackee]]

** [[https://owntracks.nshephard.dev][OwnTracks]]

I setup my own [[id:5315e7ee-0ed9-4514-b1a9-0a03114d8191][owntracks]] instance.

** Wordpress

I also (used to) host a Wordpress site with the intention of monetising my photography. Never got a single sale one day
I might update the installation and get it working again.



* Links

+ [[https://nginx.org/en/docs/][nginx docs]]

** Website Analysis

+ [[https://web-check.xyz/][webcheck.xyz]]
+ [[https://plausible.io/][Plausible]]
+ [[https://www.hitsteps.com/][Hitsteps]]
+ [[https://heapanalytics.com][Heap]]

** Blocking Bots

+ [[https://rknight.me/blog/blocking-bots-with-nginx/][Blocking Bots with Nginx • Robb Knight]]

** Search

+ [[https://pagefind.app/][Pagefind | Pagefind — Static low-bandwidth search at scale]]
