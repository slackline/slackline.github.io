:PROPERTIES:
:ID:       3cca0dfd-0c82-4685-b9ed-6314f7c8b78f
:mtime:    20251103102450 20251022135419 20250926101805 20250618134341 20241110212615 20241002091737 20241001110421 20240529091921 20240523125824 20240507153851 20240417074159 20240416215946 20240315123523 20240206174808 20240102123440 20240101130614 20231231095334 20230728104728 20230721140504 20230329222932 20230324154925 20230227200312 20230227155044 20230207144419 20230103103309 20221212233350
:ctime:    20221212233350
:END:
#+TITLE: Python Pytest
#+DATE: [2022-04-27 Wed 07:34]
#+FILETAGS: :python:programming:testing:


* Pytest

As its web-page states [[https://docs.pytest.org/en/7.0.x/][pytest: helps you write better programs]].

Often when writing tests early on you might expect them to fail, you can use the ~@pytest.mark.xfail~ fixture to mark a
test as "expect to fail" and ~pytest~ will not halt and instead carry on with the rest of the tests.

Combining ~pytest~ with [[id:d1832004-f5ee-4e28-8d83-abfe5969c283][Hypothesis]] makes for a very powerful framework.

** Configuration

You can configure pytest within a projects directory via one of a choice of files ~setup.cfg~ is sufficient for simple
use cases but its recommended more complex configurations are made via ~pyproject.toml~ or ~XXX~.

** Usage

By default ~pytest~ looks for the ~tests~ directory and runs all tests in files starting with ~test_*~ found within. A
special file ~tests/conftest.py~ can be written that defines Fixtures that are to be used across tests.

** Fixtures

PyTest provides the [[https://docs.pytest.org/en/7.1.x/how-to/fixtures.html][~@pytest.fixture~]] decorator.

#+BEGIN_SRC python :eval no
  @pytest.fixture
  def circle_coordinates() -> np.ndarray:
      """A circle for testing curvature class and methods."""
      radius = float(1)
      coordinates = np.zeros([100, 2])
      for i in np.arange(100):
          theta = 2 * math.pi / 100 * i
          x = -math.cos(theta) * radius
          y = math.sin(theta) * radius
          coordinates[i][0] = x
          coordinates[i][1] = y
      return coordinates
#+END_SRC

These can be placed in ~conftest.py~ in the top-level, but should be broken down into nested directories if fixtures are
not required globally. Nested fixtures with the same name take precedence over those defined at a higher level. Useful
examples are in [[https://stackoverflow.com/a/34520971/1444043][this post on StackOverflow]].

*** Using Fixtures

If you define a fixture in one file and use it within the same file you may find that [[id:55581960-395e-443c-bd5d-bc00c496b6ae][Pylint]] complains about ~redefining
name '<x>' from outer scope~. The simplest solution is to define your fixtures in ~conftest.py~ as this means they can
be shared across tests, but if you are defining and using fixtures within ~conftest.py~ the alternative [[https://stackoverflow.com/a/57015304/1444043][solution]] is to
set things up using the ~name="<fixturename>"~ argument to ~pytest.fixture()~ and prefix the name of the fixture.

#+begin_src python
@pytest.fixture(name="default_config")
def fixture_default_config() -> dict[str, int]:
    return {"a": 1, "b": 2}

def update_default_config(default_config: dict[str, int]) -> dict[str, int]:
    return default_config["a"] = 3
#+end_src

*** Fixtures in parametrize

You can use ~@pytest.fixture~ in ~@pytest.mark.parametrize~, there are two ways to do so (see [[https://stackoverflow.com/questions/42014484/pytest-using-fixtures-as-arguments-in-parametrize][here]]).

#+CAPTION: Using fixtures in parametrized tests
#+NAME: pytest-fixture-parametrize
#+begin_src python
  import pytest

  @pytest.mark.parametrize(
    "api_key,event_data,expected",
    [
        (API_KEY, "event_data_online", 1),
        (API_KEY, "event_data_onsite", 1),
    ],
)
def test_create_event(api_key: str, event_data: dict, expected: int) -> None:
    """Test create_event function"""
    assert isinstance(request.getfixturevalue(event_data), dict)

#+end_src

I couldn't get this to work for me, despite having imported ~pytest~. So I gave the second solution a whirl using the
[[https://github.com/tvorog/pytest-lazy-fixture][pytest-lazy-fixture]] plugin and it worked like a charm.

#+CAPTION: Using pytest-lazy-fixture in parametrized tests
#+NAME: pytest-lazyfixture-parametrize
#+begin_src python
import pytest
@pytest.mark.parametrize(
    "api_key,event_data,expected",
    [
        (API_KEY, pytest.lazy_fixture("event_data_online"), 1),
        (API_KEY, pytest.lazy_fixture("event_data_onsite"), 1),
    ],
)
def test_create_event(api_key: str, event_data: dict, expected: int) -> None:
    """Test create_event function"""
    assert isinstance(event_data, dict)
#+end_src

** Testing Exceptions

You can [[https://docs.pytest.org/en/stable/reference/reference.html#pytest.raises][test for exceptions being raised]].

#+CAPTION: Testing for exceptions
#+NAME: pytest-exceptions
#+BEGIN_SRC python :eval no
  def test_exceptions() -> None:
      with pytest.raises(KeyError):
          assert "a" in {"b": 1, "c": 2}.keys()
#+END_SRC

** Parameterisation

It is possible to write one test and [[https://docs.pytest.org/en/7.1.x/how-to/parametrize.html][paramterise]] it, that is pass it a set of different parameters for input and
expectation. It is a very simple method of increasing the coverage and range of scenarios that you are testing.

#+CAPTION: Parameterising a test using ~pytest~ fixtures.
#+NAME: pytest-fixture
#+BEGIN_SRC python :eval no
def multiply(a, b):
    return a * b

@pytest.mark.parameterize("a, b, expected",
    [pytest.param(2, 4, 8, id="two multiplied by four"),
     pytest.param(3, 9, 27, id="three multiplied by nine"),
     pytest.param(12, 10, 120, id="twelve multiplied by ten")
    ])
def test_multiply(a, b, expected):
    assert multiply(a, b) == expected
#+END_SRC

A neat use of parameterisation is to allow some tests to pass and some to fail.

#+CAPTION: Parameterising tests, some of which pass, others fail.
#+NAME: pytest-fixture-pass-fail
#+BEGIN_SRC python :eval no
  from typing import Union

  def divide(a: Union[int, float], b: Union[int, float]) -> float:
      try:
          return a / b
      except ZeroDivisionError as e:
          raise e

  @pytest.mark.parameterize("a, b, expected")
  [(10.0, 5.0, 2.0),
   (10.0, 0.0, ZeroDivisionError())]
  def test_divide(a, b, expected):
      with a, b:
          assert divide(a, b) == expected
#+END_SRC

You can also label your parametrised tests, this gives them meaningful names and means you don't have to leave comments
in the source. This is done using ~pytest.param()~ (from [[https://gist.github.com/danjac/72e00f39d53702de9c20553a941a8343][this gist]]).

#+CAPTION: Giving pararmeters in tests and ~id~ using ~pytest.param()~
#+NAME: pytest-ficture-naming
#+begin_src
@pytest.mark.parametrize(
    ("value", "exected"),
    [
        pytest.param(None, "", id="none"),
        pytest.param("", "", id="empty")
        pytest.param("test", "test", id="text")
        pytest.param("test1 test2", "test1 test2", id="text with spaces")
    ]
)
def test_some_function(value: str, expected: str) -> None:
    assert some_function(value) == exepected

#+end_src

*** What Test Am I running?

Sometimes your tests will depend on the set of parameters. You can therefore use the `id` that you set within
~pytest.param(..., id="test1")~ to determine which set of parameters are being used within your test. This is accessed
from the ~request.node.callspec.id~ attribute.

#+CAPTION: Using the ~id~ from ~pytest.param()~ within tests
#+NAME: pytest-ficture-naming
#+begin_src
@pytest.mark.parametrize(
    ("value", "exected"),
    [
        pytest.param(None, "", id="none"),
        pytest.param("", "", id="empty")
        pytest.param("test", "test", id="text")
        pytest.param("test1 test2", "test1 test2", id="text with spaces")
    ]
)
def test_some_function(value: str, expected: str) -> None:
    # Only perform the test if we are not using the 'none' or 'empty' set of parameters
    if request.node.callspec.id not in ["none", "empty"]:
        assert some_function(value) == exepected

#+end_src


*** Fixture Parameterisation

It is also possible to [[https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#fixture-parametrize][parametrise fixtures]].

#+BEGIN_SRC python :eval no
@pytest.fixture(params=[4, 10, 100])
def circle_coordinates(request) -> np.ndarray:
    """A circle for testing curvature class and methods."""
    radius = float(1)
    coordinates = np.zeros([request.param, 2])
    for i in np.arange(request.param):
        theta = 2 * math.pi / request.param * i
        x = -math.cos(theta) * radius
        y = math.sin(theta) * radius
        coordinates[i][0] = x
        coordinates[i][1] = y
    return coordinates
#+END_SRC

** Mocking

Mocking means setting up objects that return the expected value rather than having to run a lengthy, and possibly
unstable process that might not return what you expect (if for example a web-service can not be reached).

** Timing

Running tests takes time and ideally tests should run as quickly as possible. You can check how long it takes for tests
and fixtures to run using the [[https://github.com/blake-r/pytest-durations][pytest-durations]] plugin which adds the option `--durations` and prints out a nice report.

** Profiling

Profiling your code helps identify bottlenecks that can be targeted for improving performance and there are a number of
different [[id:dd7c615f-cd8b-426d-aec0-cfd3803437cc][Python Profiling]] options available to do this as a standalone exercise. However there is the [[https://pypi.org/project/pytest-profiling/][pytest-profiling]]
plugin.

After installing the package it adds a number of options to ~pytest~

| Option             | Description                                                         |
|--------------------+---------------------------------------------------------------------|
| ~--profile~        | Generate profiling information                                      |
| ~--profile-svg~    | Generate profiling graph                                            |
| ~--pstats-dir~     | Configure the dump directory of profile data files                  |
| ~--element-number~ | Defines how many elements will display in a result                  |
| ~--strip-dirs~     | Configure to show/hide the leading path information from file names |

There is, according to the documentation, a fixture which can be added to tests but I haven't worked out what that is or
how to use it just yet.

Another possibly useful plugin I came across but haven't investigated is [[https://github.com/ionelmc/pytest-benchmark][pytest-benchmark]] which allows benchmarking of
individual functions (one per test).

* Documentation Tests

I came across an article advocating [[https://simonwillison.net/2018/Jul/28/documentation-unit-tests/][Documentation unit tests]] and showing one approach to testing documentation.

** Logging

Pytest has good support for testing logging and provides the fixture ~caplog~. You can set the level at which logs are
captured by ~caplog~ on the default root logger by setting ~caplog.set_level(logging.INFO)~ but you can also do this on
a specific logger with ~caplog.set_level(logging.DEBUG, logger=LOGGER_NAME)~. Found a useful note about this on a [[https://github.com/pytest-dev/pytest/issues/7335#issuecomment-1319008772][thread
on GitHub]].

A simple example using [[https://github.com/AFM-SPM/TopoStats/blob/main/topostats/logs/logs.py][TopoStats setup_logger()]] to check debug levels is.

#+begin_src python
"""Tests for logging"""
import logging
from topostats.logs.logs import setup_logger, LOGGER_NAME


LOGGER = setup_logger(LOGGER_NAME)


def test_setup_logger(caplog) -> None:
    """Test logger setup"""
    test_info = "This is a test message"
    LOGGER.info(test_info)
    assert isinstance(LOGGER, logging.Logger)
    assert test_info in caplog.text


def test_debug(caplog) -> None:
    """Test logging debug messages."""
    test_debug = "This is a debug message"
    # Set the level of logging to DEBUG in the topostats logger
    caplog.set_level(logging.DEBUG, logger=LOGGER_NAME)
    LOGGER.debug(test_debug)
    with caplog.at_level(logging.DEBUG):
        assert isinstance(LOGGER, logging.Logger)
        assert test_debug in caplog.text
#+end_src

These tests pass.

It is worth noting two important things...

+ The log-level (i.e. ~INFO~, ~WARNING~, ~DEBUG~ etc.) is /not/ captured in ~caplog.text~ only the actual log messages
  themselves are.
+ If you have [[id:077cb9b0-a54e-45b0-abdf-1b8a5bb63aa9][multiprocessing]] in your work flow ~pytest~ does /not/ capture logging statements from spawned processes
  (see [[https://github.com/pytest-dev/pytest/issues/3037][pytest-dev/pytest · Issue #3037 · caplog fixture: capture log records from another process]]).

*** loguru

Note that loguru does [[https://github.com/Delgan/loguru/issues/59][not work with pytest caplog]], but a simple solution is provided in the [[https://loguru.readthedocs.io/en/latest/resources/migration.html#replacing-caplog-fixture-from-pytest-library][documentation]] which
redefines the ~caplog~ fixture.


* Plugins

There are lots of PyTest plugins/extensions.

** pytest-benchmark

** pytest-check

** pytest-monkeytype

[[https://github.com/mariusvniekerk/pytest-monkeytype][pytest-monkeytype]] is a plugin that facilitates running [[https://github.com/Instagram/MonkeyType][MonkeyType]] which generates run-time types of function arguments
and return values.  These can be applied to existing code to apply Typehints.

** pytest-mpl

Generate Matplotlib images and compare them in subsequent runs with [[https://pytest-mpl.readthedocs.io/en/latest/usage.html][pytest-mpl]].

** pytest-profiling

[[https://github.com/man-group/pytest-plugins][pytest-profiling]] makes it easy to profile your tests.

** pytest-testmon

[[https://github.com/tarpas/pytest-testmon/][pytest-testmon]] is an extension that allows you to run tests only on files that have been changed or on changes to
environment variables or package versions.

** pytest-regression

Another framework for regression tests [[https://github.com/ESSS/pytest-regressions][pytest-regressions]].

** pytest-regtest

Regression testing of methods, writes output to files and compares subsequent runs to these.

** inline-snapshot

This is somewhat baffling to me at the moment but [[https://15r10nk.github.io/inline-snapshot/][inline-snapshot]] has a pytest plugin extension.

** syrupy

[[https://syrupy-project.github.io/syrupy/][Syrupy]] is another pytest snapshot plugin.

* Miscellaneous

Thanks to [[https://fosstodon.org/@adamchainz/112484238160832627][this toot]] I discovered it is possible to use [[id:3c905838-8de4-4bb6-9171-98c1332456be][Git]] to pass a list of modified files to ~pytest~ to run the tests
against.

#+begin_src bash
  pytest $(git diff --name-only '*/test_*.py')
#+end_src

* Links

** Pytest

+ [[https://docs.pytest.org/en/7.0.x/][pytest: helps you write better programs]]
+ [[https://docs.pytest.org/en/7.1.x/how-to/parametrize.html][How to parametrize fixtures and test functions]]
+ [[https://docs.pytest.org/en/7.1.x/how-to/fixtures.html][How to use fixtures — pytest documentation]]
+ [[https://docs.pytest.org/en/7.1.x/how-to/logging.html][How to manage logging — pytest documentation]]
+ [[https://docs.pytest.org/en/latest/how-to/capture-stdout-stderr.html][How to capture stdout/stderr output — pytest documentation]]

*** Extensions

+ [[https://github.com/mariusvniekerk/pytest-monkeytype][pytest-monkeytype]]
+ [[https://pytest-mpl.readthedocs.io/en/latest/usage.html][pytest-mpl]]
+ [[https://github.com/man-group/pytest-plugins][pytest-profiling]]
+ [[https://github.com/ESSS/pytest-regressions][pytest-regressions]]
+ [[https://github.com/tarpas/pytest-testmon/][pytest-testmon]]
+ [[https://syrupy-project.github.io/syrupy/][Syrupy]]

** Mocking

+ [[https://docs.getmoto.org/en/latest/][Moto: Mock AWS Services — Moto 4.1.14.dev documentation]]

** Unit Testing

+ [[http://www.owenpellegrin.com/blog/testing/how-do-you-solve-multiple-asserts/][How do you solve multiple asserts?]]
+ [[https://www.artofunittesting.com][The Art of Unit Testing]]
+ [[https://softwareengineering.stackexchange.com/questions/7823/is-it-ok-to-have-multiple-asserts-in-a-single-unit-test][Is it OK to have multiple asserts in a single unit test?]] (see also [[https://softwareengineering.stackexchange.com/questions/341953/split-tests-by-method-or-behavior][unit testing - Split tests by method or behavior?]]).

** Benchmarking

+ [[https://codspeed.io/blog/one-pytest-marker-to-track-the-performance-of-your-tests][One pytest Marker to Track the Performance of Your Tests - CodSpeed]]

** Misc

+ [[https://blog.jetbrains.com/pycharm/2024/03/pytest-vs-unittest/][The JetBrains Blog Pytest vs. Unittest: Which Is Better?]]

** Books

+ [[https://www.fuzzingbook.org/][The Fuzzing Book]]
