:PROPERTIES:
:ID:       3cca0dfd-0c82-4685-b9ed-6314f7c8b78f
:mtime:    20230721140504 20230329222932 20230324154925 20230227200312 20230227155044 20230207144419 20230103103309 20221212233350
:ctime:    20221212233350
:END:
#+TITLE: Python Pytest
#+DATE: [2022-04-27 Wed 07:34]
#+FILETAGS: :python:programming:testing:

* Pytest

As its web-page states [[https://docs.pytest.org/en/7.0.x/][pytest: helps you write better programs]].

Often when writing tests early on you might expect them to fail, you can use the ~@pytest.mark.xfail~ fixture to mark a
test as "expect to fail" and ~pytest~ will not halt and instead carry on with the rest of the tests.

** Configuration

You can configure pytest within a projects directory via one of a choice of files ~setup.cfg~ is sufficient for simple
use cases but its recommended more complex configurations are made via ~pyproject.toml~ or ~XXX~.

** Usage

By default ~pytest~ looks for the ~tests~ directory and runs all tests in files starting with ~test_*~ found within. A
special file ~tests/conftest.py~ can be written that defines Fixtures that are to be used across tests.

** Fixtures

PyTest provides the [[https://docs.pytest.org/en/7.1.x/how-to/fixtures.html][~@pytest.fixture~]] decorator.

#+BEGIN_SRC python :eval no
  @pytest.fixture
  def circle_coordinates() -> np.ndarray:
      """A circle for testing curvature class and methods."""
      radius = float(1)
      coordinates = np.zeros([100, 2])
      for i in np.arange(100):
          theta = 2 * math.pi / 100 * i
          x = -math.cos(theta) * radius
          y = math.sin(theta) * radius
          coordinates[i][0] = x
          coordinates[i][1] = y
      return coordinates
#+END_SRC

*** Fixtures in parametrize

You can use ~@pytest.fixture~ in ~@pytest.mark.parametrize~, there are two ways to do so (see [[https://stackoverflow.com/questions/42014484/pytest-using-fixtures-as-arguments-in-parametrize][here]]).

#+CAPTION: Using fixtures in parametrized tests
#+NAME: pytest-fixture-parametrize
#+begin_src python
  import pytest

  @pytest.mark.parametrize(
    "api_key,event_data,expected",
    [
        (API_KEY, "event_data_online", 1),
        (API_KEY, "event_data_onsite", 1),
    ],
)
def test_create_event(api_key: str, event_data: dict, expected: int) -> None:
    """Test create_event function"""
    assert isinstance(request.getfixturevalue(event_data), dict)

#+end_src

I couldn't get this to work for me, despite having imported ~pytest~. So I gave the second solution a whirl using the
[[https://github.com/tvorog/pytest-lazy-fixture][pytest-lazy-fixture]] plugin and it worked like a charm.

#+CAPTION: Using pytest-lazy-fixture in parametrized tests
#+NAME: pytest-lazyfixture-parametrize
#+begin_src python
import pytest
@pytest.mark.parametrize(
    "api_key,event_data,expected",
    [
        (API_KEY, pytest.lazy_fixture("event_data_online"), 1),
        (API_KEY, pytest.lazy_fixture("event_data_onsite"), 1),
    ],
)
def test_create_event(api_key: str, event_data: dict, expected: int) -> None:
    """Test create_event function"""
    assert isinstance(event_data, dict)
#+end_src

** Testing Exceptions

You can [[https://docs.pytest.org/en/stable/reference/reference.html#pytest.raises][test for exceptions being raised]].

#+CAPTION: Testing for exceptions
#+NAME: pytest-exceptions
#+BEGIN_SRC python :eval no
  def test_exceptions() -> None:
      with pytest.raises(KeyError):
          assert "a" in {"b": 1, "c": 2}.keys()
#+END_SRC

** Parameterisation

It is possible to write one test and [[https://docs.pytest.org/en/7.1.x/how-to/parametrize.html][paramterise]] it, that is pass it a set of different parameters for input and
expectation. It is a very simple method of increasing the coverage and range of scenarios that you are testing.

#+CAPTION: Parameterising a test using ~pytest~ fixtures.
#+NAME: pytest-fixture
#+BEGIN_SRC python :eval no
def multiply(a, b):
    return a * b

@pytest.mark.parameterize("a, b, expected",
    [(2, 4, 8),
     (3, 9, 27),
     (12, 10, 120)
    ])
def test_multiply(a, b, expected):
    assert multiply(a, b) == expected
#+END_SRC

A neat use of parameterisation is to allow some tests to pass and some to fail.

#+CAPTION: Parameterising tests, some of which pass, others fail.
#+NAME: pytest-fixture-pass-fail
#+BEGIN_SRC python :eval no
  from typing import Union

  def divide(a: Union[int, float], b: Union[int, float]) -> float:
      try:
          return a / b
      except ZeroDivisionError as e:
          raise e

  @pytest.mark.parameterize("a, b, expected")
  [(10.0, 5.0, 2.0),
   (10.0, 0.0, ZeroDivisionError())]
  def test_divide(a, b, expected):
      with a, b:
          assert divide(a, b) == expected
#+END_SRC


*** Fixture Parameterisation

It is also possible to [[https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#fixture-parametrize][parametrise fixtures]].

#+BEGIN_SRC python :eval no
@pytest.fixture(params=[4, 10, 100])
def circle_coordinates(request) -> np.ndarray:
    """A circle for testing curvature class and methods."""
    radius = float(1)
    coordinates = np.zeros([request.param, 2])
    for i in np.arange(request.param):
        theta = 2 * math.pi / request.param * i
        x = -math.cos(theta) * radius
        y = math.sin(theta) * radius
        coordinates[i][0] = x
        coordinates[i][1] = y
    return coordinates
#+END_SRC

** Mocking

Mocking means setting up objects that return the expected value rather than having to run a lengthy, and possibly
unstable process that might not return what you expect (if for example a web-service can not be reached).

* Documentation Tests

I came across an article advocating [[https://simonwillison.net/2018/Jul/28/documentation-unit-tests/][Documentation unit tests]] and showing one approach to testing documentation.

** Logging

Pytest has good support for testing logging and provides the fixture ~caplog~. You can set the level at which logs are
captured by ~caplog~ on the default root logger by setting ~caplog.set_level(logging.INFO)~ but you can also do this on
a specific logger with ~caplog.set_level(logging.DEBUG, logger=LOGGER_NAME)~. Found a useful note about this on a [[https://github.com/pytest-dev/pytest/issues/7335#issuecomment-1319008772][thread
on GitHub]].

A simple example using [[https://github.com/AFM-SPM/TopoStats/blob/main/topostats/logs/logs.py][TopoStats setup_logger()]] to check debug levels is.

#+begin_src python
"""Tests for logging"""
import logging
from topostats.logs.logs import setup_logger, LOGGER_NAME


LOGGER = setup_logger(LOGGER_NAME)


def test_setup_logger(caplog) -> None:
    """Test logger setup"""
    test_info = "This is a test message"
    LOGGER.info(test_info)
    assert isinstance(LOGGER, logging.Logger)
    assert test_info in caplog.text


def test_debug(caplog) -> None:
    """Test logging debug messages."""
    test_debug = "This is a debug message"
    # Set the level of logging to DEBUG in the topostats logger
    caplog.set_level(logging.DEBUG, logger=LOGGER_NAME)
    LOGGER.debug(test_debug)
    with caplog.at_level(logging.DEBUG):
        assert isinstance(LOGGER, logging.Logger)
        assert test_debug in caplog.text
#+end_src

These tests pass.

It is worth noting two important things...

+ The log-level (i.e. ~INFO~, ~WARNING~, ~DEBUG~ etc.) is /not/ captured in ~caplog.text~ only the actual log messages
  themselves are.
+ If you have [[id:077cb9b0-a54e-45b0-abdf-1b8a5bb63aa9][multiprocessing]] in your work flow ~pytest~ does /not/ capture logging statements from spawned processes
  (see [[https://github.com/pytest-dev/pytest/issues/3037][pytest-dev/pytest · Issue #3037 · caplog fixture: capture log records from another process]]).

* Plugins

There are lots of PyTest plugins/extensions.

** pytest-mpl

** pytest-regtest

** pytest-benchmark

** pytest-check

** pytest-monkeytype

[[https://github.com/mariusvniekerk/pytest-monkeytype][pytest-monkeytype]] is a plugin that facilitates running [[https://github.com/Instagram/MonkeyType][MonkeyType]] which generates run-time types of function arguments
and return values.  These can be applied to existing code to apply Typehints.

* Links

+ [[https://docs.pytest.org/en/7.0.x/][pytest: helps you write better programs]]
+ [[https://docs.pytest.org/en/7.1.x/how-to/parametrize.html][How to parametrize fixtures and test functions]]
+ [[https://docs.pytest.org/en/7.1.x/how-to/fixtures.html][How to use fixtures — pytest documentation]]
+ [[https://docs.pytest.org/en/7.1.x/how-to/logging.html][How to manage logging — pytest documentation]]
+ [[https://docs.pytest.org/en/latest/how-to/capture-stdout-stderr.html][How to capture stdout/stderr output — pytest documentation]]

** Mocking

+ [[https://docs.getmoto.org/en/latest/][Moto: Mock AWS Services — Moto 4.1.14.dev documentation]]
